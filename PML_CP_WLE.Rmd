---
title: "Practical Machine Learning Peer-Graded Assessment - Weight Lifting Exercise"
author: "Aaron Schlafly"
date: "1/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
library(rattle)
```

## Background

This document is prepared as the final assignment for the Practical Machine Learning
course from Coursera. This work is not an actuarial communication, and it is 
personal work not related in any way to my employer.

The objective is to design a prediction model to classify the manner in which 20
exercises were performed based on a machine-learning algorithm trained on 19,622
classified exercises.


## Data
The dataset is licensed under the Creative Commons licenses (CC BY-SA) - citation below  

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. 
**Qualitative Activity Recognition of Weight Lifting Exercises.** 
Proceedings of 4th International Conference in Cooperation with SIGCHI 
(Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Here I read in the training data only. I wait until later to read in the testing data to avoid confusion
and to ensure the testing data aren't used before the best model is chosen based on 
the training data.

```{r read_data}
pml_training = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
dim(pml_training)
```

According to the paper mentioned above, the researchers used four 9 degrees of freedom Razor
inertial measurement units (IMU), which provide three-axes
acceleration, gyroscope and magnetometer data. The sensors were placed on the users'
glove (`forearm`), armband(`arm`), lumbar belt(`belt`), and dumbbell(`dumbbell`). 
Data were collected at a short series of timesteps, after which some key metrics were
calculated. 


The manner is classified into one correct exercise and four common mistakes, as follows:  

Class A: exactly according to specification  
Class B: throwing the elbows to the front  
Class C: lifting the dumbbell only halfway  
Class D: lowering the dumbbell only halfway  
Class E: throwing the hips to the front  

When the data set is read in, there are 160 variables, broken down as follows:    

* 100 variables ($8 \times 3 \times 4 + 4 = 96 + 4$) come from the combinations of the following:  

(In addition to the table below, the extra four variables correspond 
to `var_total_accel_`[*location*] for the four sensor locations)  

| Calculations (8) | Euler Angles (3)| Sensor Locations (4)|  
|:----------------:|:---------------:|:-------------------:|  
|`kurtosis`|`roll`|`belt`|  
|`skewness`|`pitch`|`arm`|  
|`max`|`yaw`|`dumbbell`|  
|`min`||`forearm`|  
|`amplitude`|||
|`var`|||
|`avg`|||
|`stddev`|||

* 16 variables ($(3 + 1) \times 4 = 16$) come from combinations of the following:  

| Euler Angles (3+1) | Sensor Locations (4)|  
|:----------------:|:---------------:|  
|`roll`|`belt`|  
|`pitch`|`arm`|  
|`yaw`|`dumbbell`|  
|`total`|`forearm`|  

* 36 variables ($3 \times 4 \times 3 = 36$) come from combinations of the following:  

| Sensors (3) | Sensor Locations (4)| Dimensions (3) |  
|:----------------:|:---------------:|:-------------------:|  
|`accel`|`belt`|`x`|  
|`gyros`|`arm`|`y`|  
|`magnet`|`dumbbell`|`z`|  
||`forearm`||  

* That leaves 8 remaining variables as follows:  
  + 1: `X` refers to the observation number.  
  + 1:  `user_name` identifies the participant
  + 3: `raw_timestamp_part_1`, `raw_timestamp_part_2`, and `cvtd_timestamp` are time variables  
  + 2: `new_window` and `num_window` are used to distinguish observation windows  
  + 1: The final variable is different between the two files:  
    - In `pml_training`, the variable `classe` is the classification provided in order to train the data.
    - In `pml_testing`, the variable `problem_id` is the identifier (1-20) for each of the 20 points to be classified

The first set of 100 variables only exists when a stream of observations is 
summarized. Those variables are not reliably available for predition. Therefore,
only the 16 `roll_`, `pitch_`, and `yaw_` variables plus the 36 `accel_`, `gyros_`, 
`magnet_`, and `total` variables will be used for predicting. Additionally, the eight remaining
variables are identifiers which are not relevant for new data which would come in.

Looking at the variables with near zero or zero variance, the list corresponds to the 
100 summary variables.  

```{r nearzerovariance}
sort(nearZeroVar(pml_training, names = TRUE))
```

Based on this, the study will focus on the 16 + 36 other variables.  

```{r make_small_pml_training}
pml_grep_pattern = "^roll_|^pitch_|^yaw|^total|^accel|^gyros|^magnet|classe|problem_id"
pml_pred_vars = grep(pml_grep_pattern, names(pml_training), value = TRUE)
pml_pv_count = length(pml_pred_vars)
pml_small_training = pml_training[,grep(pml_grep_pattern, names(pml_training))]
pml_small_training$classe = as.factor(pml_small_training$classe)
sort(names(pml_small_training))
```



## Data exploration

In order to determine whether there are any obvious candidates for variables to focus 
on, I looked at the distributions of groups of variables, split by classification.

```{r data_exploration}
tot_accels = pml_small_training[,c(grep("^total_accel_|classe", names(pml_small_training)))]
tot_accels %>% gather("location","total_accel",1:4) %>%
    ggplot(aes(x = total_accel, col = classe)) +
    geom_freqpoly() + facet_wrap(.~location, ncol = 2)

accels = pml_small_training[,c(grep("^accel_|classe", names(pml_small_training)))]
accels %>% gather("location","accel",1:12) %>%
    ggplot(aes(x = accel, col = classe)) +
    geom_freqpoly() + facet_wrap(.~location, ncol = 3)

```

I also looked at the 12 plots for the gyroscope and magenetometer, and the plots showed
even less distinction between classes

## Cross Validation
Although the assignment is split into training and testing data, in order to 
better understand the accuracy and the potential of over-fitting, the training data
is split into training data and validation data. The models are then trained on 
the `pml_train` subset of `"pml_training.csv"` and the `pml_valiation` will be used 
to better understand the accuracy when applied to a set other than the training set. 

```{r split_training_data}
set.seed(32343)
inTrain = createDataPartition(y = pml_small_training$classe, p = 0.7, list = FALSE)
pml_train = pml_small_training[inTrain,]
pml_validation = pml_small_training[-inTrain,]
```


## Model Selection

Since the data exploration above didn't identify any obvious candidates as variables
for a linear and given that multiple sensors working in multiple dimensions, I tested whether there 
are good combinations of variables which are better predictors. I pre-processed the data
with principal component analysis (PCA) and plotted the two strongest components to see 
whether the classes can be separated. Looking at the plot, the data seem to remain
thoroughly mixed even after rotating the vectors.

```{r pml_pca_visualization}
pml_pc = prcomp(pml_validation[,-pml_pv_count])
plot(pml_pc$x[,1], pml_pc$x[,2], col = pml_train$classe, cex = 0.2)
```

Based on this, the linear model with principal components model was skipped.

Next I tried to use a tree to allow for more complex classification. Below it can be seen that using `rpart`
for a simpler tree produces understandable results, but is a poor predictor for the 
classification.  

```{r pml_rpart}
system.time({pml_tree_fit = train(classe ~ ., data = pml_train, method = "rpart")})
print(pml_tree_fit$finalModel)
fancyRpartPlot(pml_tree_fit$finalModel)
confusionMatrix(pml_train$classe, predict(pml_tree_fit, pml_train))
confusionMatrix(pml_validation$classe, predict(pml_tree_fit, pml_validation))
```

The method seems to do a reasonably good job of correctly identifying "E" type 
problems, but cannot distinguish "D" type at all. All types of exercises are commonly 
called correct, "A"-type exercises.

For the next attempt, I trained using random forests. The major concern with this
is the time it takes. In order to speed things up, I used the `doMC` package to
allow multiple cores to be used, reducing the time.
```{r pml_rf}
library(doMC)
registerDoMC(cores = 4)

system.time({pml_rf_fit = train(classe ~ ., method = "rf", data = pml_train)})
confusionMatrix(pml_train$classe,predict(pml_rf_fit, pml_train))
confusionMatrix(pml_validation$classe, predict(pml_rf_fit, pml_validation))
```

The results work very well, making perfect predictions. When applied to the testing
data, the accuracy remains very high, but is, obviously, less than the accuracy
for the model applied to the training data.

Finally, a gradient boosing machine method is used.
```{r pml_gbm}
system.time({pml_gbm_fit = train(classe~., method = "gbm", data = pml_train, verbose = FALSE)})
print(pml_gbm_fit)
confusionMatrix(pml_train$classe,predict(pml_gbm_fit, pml_train))
confusionMatrix(pml_validation$classe,predict(pml_gbm_fit, pml_validation))
```

The model runs more quickly than it did in the case of the random forest, 
but the accuracy is slightly less, both for the training data as 
well as for the testing data.

The best prediction model seems to be the random forest version, so this will
be the version that I use to make predictions on the data in the file `"pml_testing.csv"`.  

**I predict that the accuracy will be similar to what was observed for the model 
applied to the validation data, i.e. 99.6% or so.

## Read in training data
```{r read_training}
pml_testing = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
dim(pml_testing)
```

My official prediction for the classification is based on the random forest model, 
as follows:

```{r predict_test_rf}
(test_predict_rf = predict(pml_rf_fit, pml_testing))
```

To check the consistency with the prediction from other models, I also produced the 
predictions using the tree and the gradient boosting machine: 

```{r}
rbind(rf = predict(pml_rf_fit, pml_testing),
      gbm = predict(pml_gbm_fit, pml_testing),
      tree = predict(pml_tree_fit, pml_testing))
```

The GBM, which also had a high degree of accuracy, produces identical predictions, 
which is reassuring, but the tree, which performed poorly, gives different prections, 
as expected.

